{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec4f00d-d072-4bce-9ff5-107287b00fc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/POLIZAS_NUEVO.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Cargar archivos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m polizas = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/POLIZAS_NUEVO.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFecha_Inicio_Poliza\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFecha_Fin_Poliza\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m reclamaciones = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mdata/RECLAMACIONES_NUEVO.csv\u001b[39m\u001b[33m'\u001b[39m, parse_dates=[\u001b[33m'\u001b[39m\u001b[33mFecha_Reclamacion\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      6\u001b[39m clientes = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mdata/EMPRESAS_NUEVO.csv\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Incluye ubicaciones\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/POLIZAS_NUEVO.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar archivos\n",
    "polizas = pd.read_csv('data/POLIZAS_NUEVO.csv', parse_dates=['Fecha_Inicio_Poliza', 'Fecha_Fin_Poliza'])\n",
    "reclamaciones = pd.read_csv('data/RECLAMACIONES_NUEVO.csv', parse_dates=['Fecha_Reclamacion'])\n",
    "clientes = pd.read_csv('data/EMPRESAS_NUEVO.csv')  # Incluye ubicaciones\n",
    "\n",
    "# Exportar como pickle para uso rápido\n",
    "polizas.to_pickle('data/polizas.pkl')\n",
    "reclamaciones.to_pickle('data/reclamaciones.pkl')\n",
    "clientes.to_pickle('data/clientes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f6026c2-1cb5-41bd-a745-c2aa36fedc4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m clientes = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mC:/Users/ibarr/Documents/EMPRESAS_NUEVO.csv\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Incluye ubicaciones\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Exportar como pickle para uso rápido\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mpolizas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/polizas.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m reclamaciones.to_pickle(\u001b[33m'\u001b[39m\u001b[33mdata/reclamaciones.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m clientes.to_pickle(\u001b[33m'\u001b[39m\u001b[33mdata/clientes.pkl\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:3184\u001b[39m, in \u001b[36mNDFrame.to_pickle\u001b[39m\u001b[34m(self, path, compression, protocol, storage_options)\u001b[39m\n\u001b[32m   3134\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3135\u001b[39m \u001b[33;03mPickle (serialize) object to file.\u001b[39;00m\n\u001b[32m   3136\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3180\u001b[39m \u001b[33;03m4    4    9\u001b[39;00m\n\u001b[32m   3181\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m   3182\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpickle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_pickle\n\u001b[32m-> \u001b[39m\u001b[32m3184\u001b[39m \u001b[43mto_pickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3185\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3190\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\pickle.py:103\u001b[39m, in \u001b[36mto_pickle\u001b[39m\u001b[34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m protocol < \u001b[32m0\u001b[39m:\n\u001b[32m    101\u001b[39m     protocol = pickle.HIGHEST_PROTOCOL\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# letting pickle write directly to the buffer is more memory-efficient\u001b[39;00m\n\u001b[32m    111\u001b[39m     pickle.dump(obj, handles.handle, protocol=protocol)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: 'data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar archivos\n",
    "polizas = pd.read_csv('C:/Users/ibarr/Documents/POLIZAS_NUEVO.csv', parse_dates=['Fecha_Inicio_Poliza', 'Fecha_Fin_Poliza'])\n",
    "reclamaciones = pd.read_csv('C:/Users/ibarr/Documents/RECLAMACIONES_NUEVO.csv', parse_dates=['Fecha_Reclamacion'])\n",
    "clientes = pd.read_csv('C:/Users/ibarr/Documents/EMPRESAS_NUEVO.csv')  # Incluye ubicaciones\n",
    "\n",
    "# Exportar como pickle para uso rápido\n",
    "polizas.to_pickle('data/polizas.pkl')\n",
    "reclamaciones.to_pickle('data/reclamaciones.pkl')\n",
    "clientes.to_pickle('data/clientes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff73c6bf-c867-4fb7-8634-08022a423362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Crear la carpeta 'data' si no existe\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Cargar tus archivos\n",
    "polizas = pd.read_csv('C:/Users/ibarr/Documents/POLIZAS_NUEVO.csv', parse_dates=['Fecha_Inicio_Poliza', 'Fecha_Fin_Poliza'])\n",
    "reclamaciones = pd.read_csv('C:/Users/ibarr/Documents/RECLAMACIONES_NUEVO.csv', parse_dates=['Fecha_Reclamacion'])\n",
    "clientes = pd.read_csv('C:/Users/ibarr/Documents/EMPRESAS_NUEVO.csv')\n",
    "\n",
    "# Guardar en formato pickle para carga rápida posterior\n",
    "polizas.to_pickle('data/polizas.pkl')\n",
    "reclamaciones.to_pickle('data/reclamaciones.pkl')\n",
    "clientes.to_pickle('data/clientes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c79f7d-bb4f-4483-847c-d44819ed9c7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/dataset_enriquecido.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Cargar datos enriquecidos (ej. polizas + reclamaciones cruzadas)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/dataset_enriquecido.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m X = df.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mchurn\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      9\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mchurn\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\pickle.py:185\u001b[39m, in \u001b[36mread_pickle\u001b[39m\u001b[34m(filepath_or_buffer, compression, storage_options)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m4    4    9\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m excs_to_catch = (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[32m    198\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/dataset_enriquecido.pkl'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Cargar datos enriquecidos (ej. polizas + reclamaciones cruzadas)\n",
    "df = pd.read_pickle('data/dataset_enriquecido.pkl')\n",
    "X = df.drop(columns=['churn'])\n",
    "y = df['churn']\n",
    "\n",
    "# Modelo y guardado\n",
    "model = RandomForestClassifier(max_depth=5, random_state=42)\n",
    "model.fit(X, y)\n",
    "joblib.dump(model, 'modelos/churn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1acc6065-c816-4900-9b94-a6f021b0f326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset enriquecido creado con éxito.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Crear carpeta 'data' si no existe\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Cargar los datos\n",
    "polizas = pd.read_csv('C:/Users/ibarr/Documents/POLIZAS_NUEVO.csv', parse_dates=['Fecha_Inicio_Poliza', 'Fecha_Fin_Poliza'])\n",
    "reclamaciones = pd.read_csv('C:/Users/ibarr/Documents/RECLAMACIONES_NUEVO.csv', parse_dates=['Fecha_Reclamacion'])\n",
    "clientes = pd.read_csv('C:/Users/ibarr/Documents/EMPRESAS_NUEVO.csv')\n",
    "\n",
    "# Métricas por cliente\n",
    "polizas['Duracion_Dias'] = (polizas['Fecha_Fin_Poliza'] - polizas['Fecha_Inicio_Poliza']).dt.days\n",
    "polizas_cliente = polizas.groupby('ID_Cliente').agg({\n",
    "    'ID_Seguro': 'count',\n",
    "    'Prima_Anual': ['sum', 'mean'],\n",
    "    'Deducible': 'mean',\n",
    "    'Duracion_Dias': 'mean'\n",
    "})\n",
    "polizas_cliente.columns = ['Num_Polizas', 'Prima_Total', 'Prima_Promedio', 'Deducible_Promedio', 'Duracion_Promedio']\n",
    "polizas_cliente.reset_index(inplace=True)\n",
    "\n",
    "# Reclamaciones por cliente\n",
    "reclamaciones_cliente = reclamaciones.groupby('ID_Cliente').agg({\n",
    "    'ID_Reclamacion': 'count',\n",
    "    'Monto_Reclamacion': 'sum'\n",
    "}).rename(columns={'ID_Reclamacion': 'Num_Reclamaciones', 'Monto_Reclamacion': 'Monto_Reclamado'})\n",
    "reclamaciones_cliente.reset_index(inplace=True)\n",
    "\n",
    "# Unir todo\n",
    "df = pd.merge(polizas_cliente, reclamaciones_cliente, on='ID_Cliente', how='left')\n",
    "df = pd.merge(df, clientes, on='ID_Cliente', how='left')\n",
    "\n",
    "# Reemplazar NaNs de clientes sin reclamaciones\n",
    "df['Num_Reclamaciones'] = df['Num_Reclamaciones'].fillna(0)\n",
    "df['Monto_Reclamado'] = df['Monto_Reclamado'].fillna(0)\n",
    "\n",
    "# Generar columna ficticia de churn (para demo o entrenamiento inicial)\n",
    "df['churn'] = (df['Num_Reclamaciones'] > 1) & (df['Prima_Total'] < 12000)\n",
    "df['churn'] = df['churn'].astype(int)\n",
    "\n",
    "# Guardar como pickle\n",
    "df.to_pickle('data/dataset_enriquecido.pkl')\n",
    "print(\"✅ Dataset enriquecido creado con éxito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3facb533-32f5-455c-a1c8-57b30af54ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/dataset_enriquecido.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0466c0e-aadc-4650-a2be-e88e945e60a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'TecnoSoluciones'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_14440\\992961631.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m y = df[\u001b[33m'churn'\u001b[39m]\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Modelo y guardado\u001b[39;00m\n\u001b[32m     12\u001b[39m model = RandomForestClassifier(max_depth=\u001b[32m5\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model.fit(X, y)\n\u001b[32m     14\u001b[39m joblib.dump(model, \u001b[33m'modelos/churn.pkl'\u001b[39m)\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1359\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m                 )\n\u001b[32m   1362\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    355\u001b[39m         \u001b[38;5;66;03m# Validate or convert input data\u001b[39;00m\n\u001b[32m    356\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[32m    357\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m\"sparse multilabel-indicator for y is not supported.\"\u001b[39m)\n\u001b[32m    358\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         X, y = validate_data(\n\u001b[32m    360\u001b[39m             self,\n\u001b[32m    361\u001b[39m             X,\n\u001b[32m    362\u001b[39m             y,\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2967\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"estimator\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m check_y_params:\n\u001b[32m   2968\u001b[39m                 check_y_params = {**default_check_params, **check_y_params}\n\u001b[32m   2969\u001b[39m             y = check_array(y, input_name=\u001b[33m\"y\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m             X, y = check_X_y(X, y, **check_params)\n\u001b[32m   2972\u001b[39m         out = X, y\n\u001b[32m   2973\u001b[39m \n\u001b[32m   2974\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m check_params.get(\u001b[33m\"ensure_2d\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m         )\n\u001b[32m   1365\u001b[39m \n\u001b[32m   1366\u001b[39m     ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m   1367\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     X = check_array(\n\u001b[32m   1369\u001b[39m         X,\n\u001b[32m   1370\u001b[39m         accept_sparse=accept_sparse,\n\u001b[32m   1371\u001b[39m         accept_large_sparse=accept_large_sparse,\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1050\u001b[39m                         )\n\u001b[32m   1051\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1052\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1053\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1055\u001b[39m                 raise ValueError(\n\u001b[32m   1056\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1057\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    754\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    755\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    756\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    758\u001b[39m \n\u001b[32m    759\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    760\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2164\u001b[39m             )\n\u001b[32m   2165\u001b[39m         values = self._values\n\u001b[32m   2166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2167\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2168\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2171\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'TecnoSoluciones'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Cargar datos enriquecidos (ej. polizas + reclamaciones cruzadas)\n",
    "df = pd.read_pickle('data/dataset_enriquecido.pkl')\n",
    "X = df.drop(columns=['churn'])\n",
    "y = df['churn']\n",
    "\n",
    "# Modelo y guardado\n",
    "model = RandomForestClassifier(max_depth=5, random_state=42)\n",
    "model.fit(X, y)\n",
    "joblib.dump(model, 'modelos/churn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "193a50b2-a278-4fcc-8c3c-582be08c9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar X y y\n",
    "X = df.drop(columns=['churn', 'ID_Cliente', 'Razon_Social'], errors='ignore')\n",
    "y = df['churn']\n",
    "\n",
    "# Transformar variables categóricas en dummies\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c9d8a3-e4e3-4b0d-9e3e-4d3e33536d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87        23\n",
      "           1       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.77        30\n",
      "   macro avg       0.38      0.50      0.43        30\n",
      "weighted avg       0.59      0.77      0.67        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibarr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\ibarr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\ibarr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['modelos/churn.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "modelo = RandomForestClassifier(max_depth=5, random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Validación inicial\n",
    "y_pred = modelo.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Guardar modelo entrenado\n",
    "import os\n",
    "os.makedirs('modelos', exist_ok=True)\n",
    "joblib.dump(modelo, 'modelos/churn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b99fd8d-d7d6-4b3d-9dc4-c2d1ff9bd42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        24\n",
      "           1       1.00      0.67      0.80         6\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.96      0.83      0.88        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibarr\\AppData\\Local\\Temp\\ipykernel_14440\\3818612521.py:55: UserWarning: Glyph 128269 (\\N{LEFT-POINTING MAGNIFYING GLASS}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\ibarr\\AppData\\Local\\Temp\\ipykernel_14440\\3818612521.py:56: UserWarning: Glyph 128269 (\\N{LEFT-POINTING MAGNIFYING GLASS}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig('visuals/confusion_matrix_churn.png')\n",
      "C:\\Users\\ibarr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 128269 (\\N{LEFT-POINTING MAGNIFYING GLASS}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo entrenado y guardado como 'modelos/churn.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('modelos', exist_ok=True)\n",
    "os.makedirs('visuals', exist_ok=True)\n",
    "\n",
    "# 📦 Cargar dataset enriquecido\n",
    "df = pd.read_pickle('data/dataset_enriquecido.pkl')\n",
    "\n",
    "# 🧹 Limpieza de columnas que no deben ir al modelo\n",
    "cols_excluir = ['ID_Cliente', 'Razon_Social', 'Nombre', 'RFC', 'Direccion']\n",
    "df = df.drop(columns=[col for col in cols_excluir if col in df.columns], errors='ignore')\n",
    "\n",
    "# 🧠 Separar variables predictoras y target\n",
    "y = df['churn']\n",
    "X = df.drop(columns=['churn'])\n",
    "\n",
    "# 🔄 Convertir variables categóricas en variables dummies\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 🔀 Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 🌳 Crear modelo Random Forest con ajuste para clases desbalanceadas\n",
    "modelo = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# 🔎 Predicción y reporte\n",
    "y_pred = modelo.predict(X_test)\n",
    "print(\"📊 Reporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# 📉 Matriz de confusión visual\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
    "plt.title('🔍 Matriz de Confusión - Modelo de Churn')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Real')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visuals/confusion_matrix_churn.png')\n",
    "plt.show()\n",
    "\n",
    "# 💾 Guardar modelo entrenado\n",
    "joblib.dump(modelo, 'modelos/churn.pkl')\n",
    "print(\"✅ Modelo entrenado y guardado como 'modelos/churn.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0567e0ce-95d3-44f4-ab72-ad156b70f4fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Preprocesamiento y modelo\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X = \u001b[43mdf\u001b[49m.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mTipo_Seguro\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      7\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mTipo_Seguro\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m modelo = DecisionTreeClassifier(max_depth=\u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Preprocesamiento y modelo\n",
    "X = df.drop(columns=['Tipo_Seguro'])\n",
    "y = df['Tipo_Seguro']\n",
    "modelo = DecisionTreeClassifier(max_depth=5)\n",
    "modelo.fit(X, y)\n",
    "\n",
    "# Guardar para predicción en el dashboard\n",
    "joblib.dump(modelo, 'modelos/recomendador.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34874484-6420-43e0-b60e-aac72c405242",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X = \u001b[43mdf\u001b[49m.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mTipo_Seguro\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['Tipo_Seguro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa29ec3-1900-42c9-8f3c-b5677284de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset enriquecido\n",
    "df = pd.read_pickle('data/dataset_enriquecido.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2a1124-0113-41c3-87e1-59702b4d6db0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Tipo_Seguro'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Tipo_Seguro'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Separar variables\u001b[39;00m\n\u001b[32m      8\u001b[39m X = df.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mTipo_Seguro\u001b[39m\u001b[33m'\u001b[39m], errors=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m y = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTipo_Seguro\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Tipo_Seguro'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Cargar el dataset enriquecido\n",
    "df = pd.read_pickle('data/dataset_enriquecido.pkl')\n",
    "\n",
    "# Separar variables\n",
    "X = df.drop(columns=['Tipo_Seguro'], errors='ignore')\n",
    "y = df['Tipo_Seguro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc123f0-e566-416d-9b5b-461b39abf15b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Monto_Reclamado'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Monto_Reclamado'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m df = pd.merge(df, clientes, on=\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     37\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mNum_Reclamaciones\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mNum_Reclamaciones\u001b[39m\u001b[33m'\u001b[39m].fillna(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mMonto_Reclamado\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMonto_Reclamado\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.fillna(\u001b[32m0\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Añadir columna objetivo: Tipo de seguro más frecuente\u001b[39;00m\n\u001b[32m     41\u001b[39m df = df.set_index(\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Monto_Reclamado'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar datos originales\n",
    "polizas = pd.read_csv('C:/Users/ibarr/Documents/POLIZAS_NUEVO.csv', parse_dates=['Fecha_Inicio_Poliza', 'Fecha_Fin_Poliza'])\n",
    "clientes = pd.read_csv('C:/Users/ibarr/Documents/EMPRESAS_NUEVO.csv')\n",
    "reclamaciones = pd.read_csv('C:/Users/ibarr/Documents/RECLAMACIONES_NUEVO.csv', parse_dates=['Fecha_Reclamacion'])\n",
    "\n",
    "# Obtener tipo de seguro más frecuente por cliente\n",
    "tipo_seguro_frecuente = (\n",
    "    polizas.groupby(['ID_Cliente', 'Tipo_Seguro'])\n",
    "    .size()\n",
    "    .reset_index(name='conteo')\n",
    "    .sort_values(['ID_Cliente', 'conteo'], ascending=[True, False])\n",
    "    .drop_duplicates(subset='ID_Cliente')\n",
    "    .set_index('ID_Cliente')['Tipo_Seguro']\n",
    ")\n",
    "\n",
    "# Métricas de cliente desde pólizas y reclamaciones\n",
    "polizas['Duracion_Dias'] = (polizas['Fecha_Fin_Poliza'] - polizas['Fecha_Inicio_Poliza']).dt.days\n",
    "polizas_cliente = polizas.groupby('ID_Cliente').agg({\n",
    "    'ID_Seguro': 'count',\n",
    "    'Prima_Anual': ['sum', 'mean'],\n",
    "    'Deducible': 'mean',\n",
    "    'Duracion_Dias': 'mean'\n",
    "})\n",
    "polizas_cliente.columns = ['Num_Polizas', 'Prima_Total', 'Prima_Promedio', 'Deducible_Promedio', 'Duracion_Promedio']\n",
    "polizas_cliente.reset_index(inplace=True)\n",
    "\n",
    "reclamos_cliente = reclamaciones.groupby('ID_Cliente').agg({\n",
    "    'ID_Reclamacion': 'count',\n",
    "    'Monto_Reclamacion': 'sum'\n",
    "}).rename(columns={'ID_Reclamacion': 'Num_Reclamaciones', 'Monto_Reclamado': 'Monto_Reclamado'}).reset_index()\n",
    "\n",
    "# Unir todo\n",
    "df = pd.merge(polizas_cliente, reclamos_cliente, on='ID_Cliente', how='left')\n",
    "df = pd.merge(df, clientes, on='ID_Cliente', how='left')\n",
    "df['Num_Reclamaciones'] = df['Num_Reclamaciones'].fillna(0)\n",
    "df['Monto_Reclamado'] = df['Monto_Reclamado'].fillna(0)\n",
    "\n",
    "# Añadir columna objetivo: Tipo de seguro más frecuente\n",
    "df = df.set_index('ID_Cliente')\n",
    "df['Tipo_Seguro'] = tipo_seguro_frecuente\n",
    "df = df.reset_index()\n",
    "\n",
    "# Guardar\n",
    "df.to_pickle('data/dataset_enriquecido.pkl')\n",
    "print(\"✅ Dataset enriquecido actualizado con columna 'Tipo_Seguro'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46d09f31-5958-489e-97cc-1da5d4d39136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset enriquecido creado exitosamente en: data/dataset_enriquecido.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# 📦 Rutas de entrada (ajusta si están en otra ubicación)\n",
    "ruta_polizas = 'C:/Users/ibarr/Documents/POLIZAS_NUEVO.csv'\n",
    "ruta_reclamos = 'C:/Users/ibarr/Documents/RECLAMACIONES_NUEVO.csv'\n",
    "ruta_clientes = 'C:/Users/ibarr/Documents/EMPRESAS_NUEVO.csv'\n",
    "\n",
    "# 🧾 Cargar datos con protección contra errores\n",
    "try:\n",
    "    polizas = pd.read_csv(ruta_polizas, parse_dates=['Fecha_Inicio_Poliza', 'Fecha_Fin_Poliza'])\n",
    "    reclamaciones = pd.read_csv(ruta_reclamos, parse_dates=['Fecha_Reclamacion'])\n",
    "    clientes = pd.read_csv(ruta_clientes)\n",
    "except Exception as e:\n",
    "    print(\"❌ Error al cargar los archivos:\", e)\n",
    "    exit()\n",
    "\n",
    "# 📌 Crear columna de duración de póliza\n",
    "polizas['Duracion_Dias'] = (polizas['Fecha_Fin_Poliza'] - polizas['Fecha_Inicio_Poliza']).dt.days\n",
    "\n",
    "# 📊 Métricas de pólizas por cliente\n",
    "polizas_cliente = polizas.groupby('ID_Cliente').agg({\n",
    "    'ID_Seguro': 'count',\n",
    "    'Prima_Anual': ['sum', 'mean'],\n",
    "    'Deducible': 'mean',\n",
    "    'Duracion_Dias': 'mean'\n",
    "})\n",
    "polizas_cliente.columns = ['Num_Polizas', 'Prima_Total', 'Prima_Promedio', 'Deducible_Promedio', 'Duracion_Promedio']\n",
    "polizas_cliente.reset_index(inplace=True)\n",
    "\n",
    "# 💥 Métricas de reclamaciones por cliente\n",
    "reclamos_cliente = reclamaciones.groupby('ID_Cliente').agg({\n",
    "    'ID_Reclamacion': 'count',\n",
    "    'Monto_Reclamacion': 'sum'\n",
    "}).rename(columns={'ID_Reclamacion': 'Num_Reclamaciones', 'Monto_Reclamacion': 'Monto_Reclamado'}).reset_index()\n",
    "\n",
    "# 🧬 Unir datasets\n",
    "df = pd.merge(polizas_cliente, reclamos_cliente, on='ID_Cliente', how='left')\n",
    "df = pd.merge(df, clientes, on='ID_Cliente', how='left')\n",
    "\n",
    "# 🧹 Reemplazar nulos de reclamos\n",
    "df['Num_Reclamaciones'] = df['Num_Reclamaciones'].fillna(0)\n",
    "df['Monto_Reclamado'] = df['Monto_Reclamado'].fillna(0)\n",
    "\n",
    "# 🧠 Generar columna de churn ficticio si no existe (puedes ajustar esta lógica real)\n",
    "if 'churn' not in df.columns:\n",
    "    df['churn'] = ((df['Num_Reclamaciones'] > 1) & (df['Prima_Total'] < 12000)).astype(int)\n",
    "\n",
    "# 🎯 Obtener tipo de seguro más frecuente por cliente (como objetivo para modelo de recomendación)\n",
    "if 'Tipo_Seguro' not in df.columns and 'Tipo_Seguro' in polizas.columns:\n",
    "    tipo_seguro = (\n",
    "        polizas.groupby(['ID_Cliente', 'Tipo_Seguro'])\n",
    "        .size()\n",
    "        .reset_index(name='conteo')\n",
    "        .sort_values(['ID_Cliente', 'conteo'], ascending=[True, False])\n",
    "        .drop_duplicates('ID_Cliente')\n",
    "        .set_index('ID_Cliente')['Tipo_Seguro']\n",
    "    )\n",
    "    df = df.set_index('ID_Cliente')\n",
    "    df['Tipo_Seguro'] = tipo_seguro\n",
    "    df = df.reset_index()\n",
    "else:\n",
    "    print(\"⚠️ No se pudo generar 'Tipo_Seguro' porque la columna no estaba en 'polizas'.\")\n",
    "\n",
    "# 💾 Guardar como pickle\n",
    "try:\n",
    "    df.to_pickle('data/dataset_enriquecido.pkl')\n",
    "    print(\"✅ Dataset enriquecido creado exitosamente en: data/dataset_enriquecido.pkl\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error al guardar el archivo enriquecido:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717f14e9-8aeb-438b-80eb-fe6778f3e739",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m modelo.fit(X, y)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Evaluación rápida\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m y_pred = modelo.predict(X_test)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📊 Reporte del modelo de recomendación:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2940\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2936\u001b[39m         CVClass = ShuffleSplit\n\u001b[32m   2938\u001b[39m     cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)\n\u001b[32m-> \u001b[39m\u001b[32m2940\u001b[39m     train, test = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2945\u001b[39m     chain.from_iterable(\n\u001b[32m   2946\u001b[39m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2947\u001b[39m     )\n\u001b[32m   2948\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:1927\u001b[39m, in \u001b[36mBaseShuffleSplit.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   1897\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[32m   1898\u001b[39m \n\u001b[32m   1899\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1924\u001b[39m \u001b[33;03mto an integer.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1926\u001b[39m X, y, groups = indexable(X, y, groups)\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2342\u001b[39m, in \u001b[36mStratifiedShuffleSplit._iter_indices\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   2340\u001b[39m class_counts = np.bincount(y_indices)\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.min(class_counts) < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2342\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2343\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe least populated class in y has only 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2344\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m member, which is too few. The minimum\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2345\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m number of groups for any class cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m be less than 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2347\u001b[39m     )\n\u001b[32m   2349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train < n_classes:\n\u001b[32m   2350\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2351\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m should be greater or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2352\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m % (n_train, n_classes)\n\u001b[32m   2353\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Crear carpeta para modelos si no existe\n",
    "os.makedirs('modelos', exist_ok=True)\n",
    "\n",
    "# Cargar dataset enriquecido\n",
    "try:\n",
    "    df = pd.read_pickle('data/dataset_enriquecido.pkl')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"❌ No se encontró el archivo 'data/dataset_enriquecido.pkl'. Asegúrate de generarlo antes.\")\n",
    "\n",
    "# Validar que exista la columna objetivo\n",
    "if 'Tipo_Seguro' not in df.columns:\n",
    "    raise ValueError(\"❌ El dataset no tiene la columna 'Tipo_Seguro'. Verifica que haya sido creada correctamente.\")\n",
    "\n",
    "# Separar variables predictoras (X) y objetivo (y)\n",
    "X = df.drop(columns=['Tipo_Seguro', 'ID_Cliente', 'Razon_Social'], errors='ignore')\n",
    "y = df['Tipo_Seguro']\n",
    "\n",
    "# Codificar variables categóricas\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "modelo = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "modelo.fit(X, y)\n",
    "\n",
    "# Evaluación rápida\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "y_pred = modelo.predict(X_test)\n",
    "print(\"📊 Reporte del modelo de recomendación:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Guardar modelo entrenado\n",
    "joblib.dump(modelo, 'modelos/recomendador.pkl')\n",
    "print(\"✅ Modelo de recomendación guardado en: modelos/recomendador.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d2a6a3-e889-4680-b158-d7d737a05808",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'TecnoSoluciones'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_6544\\2602910673.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Preprocesamiento y modelo\u001b[39;00m\n\u001b[32m      6\u001b[39m X = df.drop(columns=[\u001b[33m'Tipo_Seguro'\u001b[39m])\n\u001b[32m      7\u001b[39m y = df[\u001b[33m'Tipo_Seguro'\u001b[39m]\n\u001b[32m      8\u001b[39m modelo = DecisionTreeClassifier(max_depth=\u001b[32m5\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m modelo.fit(X, y)\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Guardar para predicción en el dashboard\u001b[39;00m\n\u001b[32m     12\u001b[39m joblib.dump(modelo, \u001b[33m'modelos/recomendador.pkl'\u001b[39m)\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1359\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m                 )\n\u001b[32m   1362\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1020\u001b[39m         self : DecisionTreeClassifier\n\u001b[32m   1021\u001b[39m             Fitted estimator.\n\u001b[32m   1022\u001b[39m         \"\"\"\n\u001b[32m   1023\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m         super()._fit(\n\u001b[32m   1025\u001b[39m             X,\n\u001b[32m   1026\u001b[39m             y,\n\u001b[32m   1027\u001b[39m             sample_weight=sample_weight,\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    248\u001b[39m             check_X_params = dict(\n\u001b[32m    249\u001b[39m                 dtype=DTYPE, accept_sparse=\u001b[33m\"csc\"\u001b[39m, ensure_all_finite=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    250\u001b[39m             )\n\u001b[32m    251\u001b[39m             check_y_params = dict(ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m             X, y = validate_data(\n\u001b[32m    253\u001b[39m                 self, X, y, validate_separately=(check_X_params, check_y_params)\n\u001b[32m    254\u001b[39m             )\n\u001b[32m    255\u001b[39m \n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2962\u001b[39m             \u001b[38;5;66;03m# :(\u001b[39;00m\n\u001b[32m   2963\u001b[39m             check_X_params, check_y_params = validate_separately\n\u001b[32m   2964\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"estimator\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m check_X_params:\n\u001b[32m   2965\u001b[39m                 check_X_params = {**default_check_params, **check_X_params}\n\u001b[32m-> \u001b[39m\u001b[32m2966\u001b[39m             X = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_X_params)\n\u001b[32m   2967\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"estimator\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m check_y_params:\n\u001b[32m   2968\u001b[39m                 check_y_params = {**default_check_params, **check_y_params}\n\u001b[32m   2969\u001b[39m             y = check_array(y, input_name=\u001b[33m\"y\"\u001b[39m, **check_y_params)\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1050\u001b[39m                         )\n\u001b[32m   1051\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1052\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1053\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1055\u001b[39m                 raise ValueError(\n\u001b[32m   1056\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1057\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    754\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    755\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    756\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    758\u001b[39m \n\u001b[32m    759\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    760\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2164\u001b[39m             )\n\u001b[32m   2165\u001b[39m         values = self._values\n\u001b[32m   2166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2167\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2168\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2171\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'TecnoSoluciones'"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Preprocesamiento y modelo\n",
    "X = df.drop(columns=['Tipo_Seguro'])\n",
    "y = df['Tipo_Seguro']\n",
    "modelo = DecisionTreeClassifier(max_depth=5)\n",
    "modelo.fit(X, y)\n",
    "\n",
    "# Guardar para predicción en el dashboard\n",
    "joblib.dump(modelo, 'modelos/recomendador.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b967f348-6b5a-49de-8c91-4b784abffe2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m X = pd.get_dummies(X, drop_first=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 5. Entrenar el modelo de árbol\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m modelo = DecisionTreeClassifier(max_depth=\u001b[32m5\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     28\u001b[39m modelo.fit(X_train, y_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2940\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2936\u001b[39m         CVClass = ShuffleSplit\n\u001b[32m   2938\u001b[39m     cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)\n\u001b[32m-> \u001b[39m\u001b[32m2940\u001b[39m     train, test = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2945\u001b[39m     chain.from_iterable(\n\u001b[32m   2946\u001b[39m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2947\u001b[39m     )\n\u001b[32m   2948\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:1927\u001b[39m, in \u001b[36mBaseShuffleSplit.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   1897\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[32m   1898\u001b[39m \n\u001b[32m   1899\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1924\u001b[39m \u001b[33;03mto an integer.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1926\u001b[39m X, y, groups = indexable(X, y, groups)\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2342\u001b[39m, in \u001b[36mStratifiedShuffleSplit._iter_indices\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   2340\u001b[39m class_counts = np.bincount(y_indices)\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.min(class_counts) < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2342\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2343\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe least populated class in y has only 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2344\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m member, which is too few. The minimum\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2345\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m number of groups for any class cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m be less than 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2347\u001b[39m     )\n\u001b[32m   2349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train < n_classes:\n\u001b[32m   2350\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2351\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m should be greater or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2352\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m % (n_train, n_classes)\n\u001b[32m   2353\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs('modelos', exist_ok=True)\n",
    "\n",
    "# 1. Cargar el dataset enriquecido\n",
    "df = pd.read_pickle('data/dataset_enriquecido.pkl')\n",
    "\n",
    "# 2. Validar que exista la columna objetivo\n",
    "if 'Tipo_Seguro' not in df.columns:\n",
    "    raise ValueError(\"❌ Falta la columna 'Tipo_Seguro' en el dataset enriquecido.\")\n",
    "\n",
    "# 3. Separar variables predictoras y etiqueta\n",
    "X = df.drop(columns=['Tipo_Seguro', 'ID_Cliente', 'Razon_Social'], errors='ignore')\n",
    "y = df['Tipo_Seguro']\n",
    "\n",
    "# 4. Convertir variables categóricas en variables numéricas\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 5. Entrenar el modelo de árbol\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "modelo = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluar el modelo\n",
    "y_pred = modelo.predict(X_test)\n",
    "print(\"📊 Reporte de clasificación:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# 7. Guardar el modelo entrenado\n",
    "joblib.dump(modelo, 'modelos/recomendador.pkl')\n",
    "print(\"✅ Modelo guardado en: modelos/recomendador.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e62827-18bc-4ad6-8e70-bd7854626926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Reporte de clasificación:\n",
      "\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                Daños       0.52      1.00      0.69        12\n",
      "             Incendio       0.67      0.22      0.33         9\n",
      "Responsabilidad Civil       0.50      0.22      0.31         9\n",
      "\n",
      "             accuracy                           0.53        30\n",
      "            macro avg       0.56      0.48      0.44        30\n",
      "         weighted avg       0.56      0.53      0.47        30\n",
      "\n",
      "✅ Modelo guardado en modelos/recomendador.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Cargar el dataset enriquecido\n",
    "df = pd.read_pickle('data/dataset_enriquecido.pkl')\n",
    "\n",
    "# Preparar X y y\n",
    "X = df.drop(columns=['Tipo_Seguro', 'ID_Cliente', 'Razon_Social'], errors='ignore')\n",
    "y = df['Tipo_Seguro']\n",
    "\n",
    "# Convertir variables categóricas a numéricas\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Eliminar clases con menos de 2 observaciones para permitir stratify\n",
    "clases_validas = y.value_counts()[y.value_counts() >= 2].index\n",
    "X = X[y.isin(clases_validas)]\n",
    "y = y[y.isin(clases_validas)]\n",
    "\n",
    "# Separar entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "modelo = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar\n",
    "y_pred = modelo.predict(X_test)\n",
    "print(\"📊 Reporte de clasificación:\\n\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Guardar el modelo\n",
    "os.makedirs('modelos', exist_ok=True)\n",
    "joblib.dump(modelo, 'modelos/recomendador.pkl')\n",
    "print(\"✅ Modelo guardado en modelos/recomendador.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec2afeb3-6f9c-435b-bf42-b11f986dedad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "❌ La columna 'Estado' no se encuentra en polizas.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df, nombre \u001b[38;5;129;01min\u001b[39;00m [(polizas, \u001b[33m'\u001b[39m\u001b[33mpolizas\u001b[39m\u001b[33m'\u001b[39m), (reclamos, \u001b[33m'\u001b[39m\u001b[33mreclamos\u001b[39m\u001b[33m'\u001b[39m)]:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mEstado\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m❌ La columna \u001b[39m\u001b[33m'\u001b[39m\u001b[33mEstado\u001b[39m\u001b[33m'\u001b[39m\u001b[33m no se encuentra en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnombre\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33mEstado\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mEstado\u001b[39m\u001b[33m'\u001b[39m].fillna(\u001b[33m'\u001b[39m\u001b[33mSIN_ESTADO\u001b[39m\u001b[33m'\u001b[39m).astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 📊 Agrupación de ventas por estado\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: ❌ La columna 'Estado' no se encuentra en polizas."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 📁 Asegurar que la carpeta de salida existe\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# 📦 Cargar archivos pickle\n",
    "clientes = pd.read_pickle('data/clientes.pkl')\n",
    "polizas = pd.read_pickle('data/polizas.pkl')\n",
    "reclamos = pd.read_pickle('data/reclamaciones.pkl')\n",
    "\n",
    "# 🛡 Asegurar que la columna 'Estado' existe y es válida\n",
    "for df, nombre in [(polizas, 'polizas'), (reclamos, 'reclamos')]:\n",
    "    if 'Estado' not in df.columns:\n",
    "        raise ValueError(f\"❌ La columna 'Estado' no se encuentra en {nombre}.\")\n",
    "    df['Estado'] = df['Estado'].fillna('SIN_ESTADO').astype(str)\n",
    "\n",
    "# 📊 Agrupación de ventas por estado\n",
    "ventas = polizas.groupby('Estado')['ID_Seguro'].count().reset_index(name='Polizas')\n",
    "\n",
    "# 📊 Agrupación de siniestros por estado\n",
    "siniestros = reclamos.groupby('Estado')['ID_Reclamacion'].count().reset_index(name='Reclamaciones')\n",
    "\n",
    "# 🔗 Unión y cálculo de tasa\n",
    "zonas = pd.merge(ventas, siniestros, on='Estado', how='outer').fillna(0)\n",
    "zonas['Polizas'] = zonas['Polizas'].astype(int)\n",
    "zonas['Reclamaciones'] = zonas['Reclamaciones'].astype(int)\n",
    "zonas['Tasa_Siniestro'] = zonas.apply(\n",
    "    lambda row: row['Reclamaciones'] / row['Polizas'] if row['Polizas'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 💾 Guardar el resultado\n",
    "zonas.to_csv('data/mapa_calor.csv', index=False)\n",
    "print(\"✅ Análisis por estado generado exitosamente en: data/mapa_calor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe93e888-011a-407d-a714-b497549cfddd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Estado'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m reclamos = pd.read_pickle(\u001b[33m'\u001b[39m\u001b[33mdata/reclamaciones.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Unir estado del cliente a cada póliza\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m polizas = pd.merge(polizas, \u001b[43mclientes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mID_Cliente\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEstado\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, on=\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m reclamos = pd.merge(reclamos, clientes[[\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mEstado\u001b[39m\u001b[33m'\u001b[39m]], on=\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Reemplazar valores nulos y convertir a texto\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['Estado'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Cargar archivos\n",
    "clientes = pd.read_pickle('data/clientes.pkl')\n",
    "polizas = pd.read_pickle('data/polizas.pkl')\n",
    "reclamos = pd.read_pickle('data/reclamaciones.pkl')\n",
    "\n",
    "# Unir estado del cliente a cada póliza\n",
    "polizas = pd.merge(polizas, clientes[['ID_Cliente', 'Estado']], on='ID_Cliente', how='left')\n",
    "reclamos = pd.merge(reclamos, clientes[['ID_Cliente', 'Estado']], on='ID_Cliente', how='left')\n",
    "\n",
    "# Reemplazar valores nulos y convertir a texto\n",
    "polizas['Estado'] = polizas['Estado'].fillna('SIN_ESTADO').astype(str)\n",
    "reclamos['Estado'] = reclamos['Estado'].fillna('SIN_ESTADO').astype(str)\n",
    "\n",
    "# Agrupar\n",
    "ventas = polizas.groupby('Estado')['ID_Seguro'].count().reset_index(name='Polizas')\n",
    "siniestros = reclamos.groupby('Estado')['ID_Reclamacion'].count().reset_index(name='Reclamaciones')\n",
    "\n",
    "# Calcular tasa\n",
    "zonas = pd.merge(ventas, siniestros, on='Estado', how='outer').fillna(0)\n",
    "zonas['Polizas'] = zonas['Polizas'].astype(int)\n",
    "zonas['Reclamaciones'] = zonas['Reclamaciones'].astype(int)\n",
    "zonas['Tasa_Siniestro'] = zonas.apply(\n",
    "    lambda row: row['Reclamaciones'] / row['Polizas'] if row['Polizas'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Guardar archivo final\n",
    "zonas.to_csv('data/mapa_calor.csv', index=False)\n",
    "print(\"✅ Archivo generado correctamente en: data/mapa_calor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06670871-82a1-4951-af58-32ac20833601",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Delegación_Municipio'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m reclamos = pd.read_pickle(\u001b[33m'\u001b[39m\u001b[33mdata/reclamaciones.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Unir la columna 'Delegación_Municipio' desde clientes a pólizas y reclamaciones\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m polizas = pd.merge(polizas, \u001b[43mclientes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mID_Cliente\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDelegación_Municipio\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, on=\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m reclamos = pd.merge(reclamos, clientes[[\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDelegación_Municipio\u001b[39m\u001b[33m'\u001b[39m]], on=\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Limpiar valores faltantes y convertir a texto\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['Delegación_Municipio'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Cargar archivos pickle\n",
    "clientes = pd.read_pickle('data/clientes.pkl')\n",
    "polizas = pd.read_pickle('data/polizas.pkl')\n",
    "reclamos = pd.read_pickle('data/reclamaciones.pkl')\n",
    "\n",
    "# Unir la columna 'Delegación_Municipio' desde clientes a pólizas y reclamaciones\n",
    "polizas = pd.merge(polizas, clientes[['ID_Cliente', 'Delegación_Municipio']], on='ID_Cliente', how='left')\n",
    "reclamos = pd.merge(reclamos, clientes[['ID_Cliente', 'Delegación_Municipio']], on='ID_Cliente', how='left')\n",
    "\n",
    "# Limpiar valores faltantes y convertir a texto\n",
    "polizas['Delegación_Municipio'] = polizas['Delegación_Municipio'].fillna('SIN_DELEGACIÓN').astype(str)\n",
    "reclamos['Delegación_Municipio'] = reclamos['Delegación_Municipio'].fillna('SIN_DELEGACIÓN').astype(str)\n",
    "\n",
    "# Agrupar por delegación\n",
    "ventas = polizas.groupby('Delegación_Municipio')['ID_Seguro'].count().reset_index(name='Polizas')\n",
    "siniestros = reclamos.groupby('Delegación_Municipio')['ID_Reclamacion'].count().reset_index(name='Reclamaciones')\n",
    "\n",
    "# Unir y calcular tasa\n",
    "zonas = pd.merge(ventas, siniestros, on='Delegación_Municipio', how='outer').fillna(0)\n",
    "zonas['Polizas'] = zonas['Polizas'].astype(int)\n",
    "zonas['Reclamaciones'] = zonas['Reclamaciones'].astype(int)\n",
    "zonas['Tasa_Siniestro'] = zonas.apply(\n",
    "    lambda row: row['Reclamaciones'] / row['Polizas'] if row['Polizas'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Guardar resultado\n",
    "zonas.to_csv('data/mapa_calor.csv', index=False)\n",
    "print(\"✅ Análisis por delegación generado correctamente: data/mapa_calor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e3a0530-f082-4ca6-a499-a5ce9fa587cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "❌ La columna 'Delegación_Municipio' no existe en EMPRESAS_NUEVO.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Confirmar que la columna existe\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mDelegación_Municipio\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m empresas.columns:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m❌ La columna \u001b[39m\u001b[33m'\u001b[39m\u001b[33mDelegación_Municipio\u001b[39m\u001b[33m'\u001b[39m\u001b[33m no existe en EMPRESAS_NUEVO.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Unir Delegación a pólizas y reclamos por ID_Cliente\u001b[39;00m\n\u001b[32m     17\u001b[39m polizas = pd.merge(polizas, empresas[[\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDelegación_Municipio\u001b[39m\u001b[33m'\u001b[39m]], on=\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: ❌ La columna 'Delegación_Municipio' no existe en EMPRESAS_NUEVO.csv"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Cargar bases\n",
    "empresas = pd.read_csv('C:/Users/ibarr/Documents/EMPRESAS_NUEVO.csv')\n",
    "polizas = pd.read_pickle('data/polizas.pkl')\n",
    "reclamos = pd.read_pickle('data/reclamaciones.pkl')\n",
    "\n",
    "# Confirmar que la columna existe\n",
    "if 'Delegación_Municipio' not in empresas.columns:\n",
    "    raise ValueError(\"❌ La columna 'Delegación_Municipio' no existe en EMPRESAS_NUEVO.csv\")\n",
    "\n",
    "# Unir Delegación a pólizas y reclamos por ID_Cliente\n",
    "polizas = pd.merge(polizas, empresas[['ID_Cliente', 'Delegación_Municipio']], on='ID_Cliente', how='left')\n",
    "reclamos = pd.merge(reclamos, empresas[['ID_Cliente', 'Delegación_Municipio']], on='ID_Cliente', how='left')\n",
    "\n",
    "# Limpiar valores nulos\n",
    "polizas['Delegación_Municipio'] = polizas['Delegación_Municipio'].fillna('SIN_DELEGACIÓN').astype(str)\n",
    "reclamos['Delegación_Municipio'] = reclamos['Delegación_Municipio'].fillna('SIN_DELEGACIÓN').astype(str)\n",
    "\n",
    "# Agrupar por delegación\n",
    "ventas = polizas.groupby('Delegación_Municipio')['ID_Seguro'].count().reset_index(name='Polizas')\n",
    "siniestros = reclamos.groupby('Delegación_Municipio')['ID_Reclamacion'].count().reset_index(name='Reclamaciones')\n",
    "\n",
    "# Calcular tasa\n",
    "zonas = pd.merge(ventas, siniestros, on='Delegación_Municipio', how='outer').fillna(0)\n",
    "zonas['Polizas'] = zonas['Polizas'].astype(int)\n",
    "zonas['Reclamaciones'] = zonas['Reclamaciones'].astype(int)\n",
    "zonas['Tasa_Siniestro'] = zonas.apply(\n",
    "    lambda row: row['Reclamaciones'] / row['Polizas'] if row['Polizas'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Guardar resultado\n",
    "zonas.to_csv('data/mapa_calor.csv', index=False)\n",
    "print(\"✅ Análisis por delegación generado correctamente: data/mapa_calor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e3f169e-93a3-4596-acf1-ad229f0e7cae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "❌ La columna 'Delegación_Municipio' no se encontró en EMPRESAS_NUEVO.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Asegurar que la columna existe\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mDelegación_Municipio\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m empresas.columns:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m❌ La columna \u001b[39m\u001b[33m'\u001b[39m\u001b[33mDelegación_Municipio\u001b[39m\u001b[33m'\u001b[39m\u001b[33m no se encontró en EMPRESAS_NUEVO.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Unir delegación con pólizas y reclamaciones por ID_Cliente\u001b[39;00m\n\u001b[32m     17\u001b[39m polizas = pd.merge(polizas, empresas[[\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDelegación_Municipio\u001b[39m\u001b[33m'\u001b[39m]], on=\u001b[33m'\u001b[39m\u001b[33mID_Cliente\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: ❌ La columna 'Delegación_Municipio' no se encontró en EMPRESAS_NUEVO.csv"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Crear carpeta de salida si no existe\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Cargar bases\n",
    "empresas = pd.read_csv('C:/Users/ibarr/Documents/EMPRESAS_NUEVO.csv')\n",
    "polizas = pd.read_pickle('data/polizas.pkl')\n",
    "reclamos = pd.read_pickle('data/reclamaciones.pkl')\n",
    "\n",
    "# Asegurar que la columna existe\n",
    "if 'Delegación_Municipio' not in empresas.columns:\n",
    "    raise ValueError(\"❌ La columna 'Delegación_Municipio' no se encontró en EMPRESAS_NUEVO.csv\")\n",
    "\n",
    "# Unir delegación con pólizas y reclamaciones por ID_Cliente\n",
    "polizas = pd.merge(polizas, empresas[['ID_Cliente', 'Delegación_Municipio']], on='ID_Cliente', how='left')\n",
    "reclamos = pd.merge(reclamos, empresas[['ID_Cliente', 'Delegación_Municipio']], on='ID_Cliente', how='left')\n",
    "\n",
    "# Limpiar y normalizar texto\n",
    "polizas['Delegación_Municipio'] = polizas['Delegación_Municipio'].fillna('SIN_DELEGACIÓN').astype(str).str.upper().str.strip()\n",
    "reclamos['Delegación_Municipio'] = reclamos['Delegación_Municipio'].fillna('SIN_DELEGACIÓN').astype(str).str.upper().str.strip()\n",
    "\n",
    "# Agrupar\n",
    "ventas = polizas.groupby('Delegación_Municipio')['ID_Seguro'].count().reset_index(name='Polizas')\n",
    "siniestros = reclamos.groupby('Delegación_Municipio')['ID_Reclamacion'].count().reset_index(name='Reclamaciones')\n",
    "\n",
    "# Combinar y calcular tasa\n",
    "zonas = pd.merge(ventas, siniestros, on='Delegación_Municipio', how='outer').fillna(0)\n",
    "zonas['Polizas'] = zonas['Polizas'].astype(int)\n",
    "zonas['Reclamaciones'] = zonas['Reclamaciones'].astype(int)\n",
    "zonas['Tasa_Siniestro'] = zonas.apply(\n",
    "    lambda row: row['Reclamaciones'] / row['Polizas'] if row['Polizas'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Guardar resultado\n",
    "zonas.to_csv('data/mapa_calor.csv', index=False)\n",
    "print(\"✅ Archivo generado exitosamente: data/mapa_calor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "202b2a7e-445f-484d-a090-aa9014c978c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo generado correctamente en: data/mapa_calor.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Crear carpeta de salida si no existe\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Cargar archivos\n",
    "empresas = pd.read_csv('C:/Users/ibarr/Documents/EMPRESAS_NUEVO.csv')\n",
    "polizas = pd.read_pickle('data/polizas.pkl')\n",
    "reclamos = pd.read_pickle('data/reclamaciones.pkl')\n",
    "\n",
    "# Validar que la columna Delegacion_Municipio existe\n",
    "if 'Delegacion_Municipio' not in empresas.columns:\n",
    "    raise ValueError(\"❌ La columna 'Delegacion_Municipio' no existe en EMPRESAS_NUEVO.csv\")\n",
    "\n",
    "# Unir delegacion a polizas y reclamos por ID_Cliente\n",
    "polizas = pd.merge(polizas, empresas[['ID_Cliente', 'Delegacion_Municipio']], on='ID_Cliente', how='left')\n",
    "reclamos = pd.merge(reclamos, empresas[['ID_Cliente', 'Delegacion_Municipio']], on='ID_Cliente', how='left')\n",
    "\n",
    "# Limpiar y estandarizar texto\n",
    "polizas['Delegacion_Municipio'] = polizas['Delegacion_Municipio'].fillna('SIN_DELEGACION').astype(str).str.upper().str.strip()\n",
    "reclamos['Delegacion_Municipio'] = reclamos['Delegacion_Municipio'].fillna('SIN_DELEGACION').astype(str).str.upper().str.strip()\n",
    "\n",
    "# Agrupar por delegacion\n",
    "ventas = polizas.groupby('Delegacion_Municipio')['ID_Seguro'].count().reset_index(name='Polizas')\n",
    "siniestros = reclamos.groupby('Delegacion_Municipio')['ID_Reclamacion'].count().reset_index(name='Reclamaciones')\n",
    "\n",
    "# Calcular tasa\n",
    "zonas = pd.merge(ventas, siniestros, on='Delegacion_Municipio', how='outer').fillna(0)\n",
    "zonas['Polizas'] = zonas['Polizas'].astype(int)\n",
    "zonas['Reclamaciones'] = zonas['Reclamaciones'].astype(int)\n",
    "zonas['Tasa_Siniestro'] = zonas.apply(\n",
    "    lambda row: row['Reclamaciones'] / row['Polizas'] if row['Polizas'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Guardar resultado\n",
    "zonas.to_csv('data/mapa_calor.csv', index=False)\n",
    "print(\"✅ Archivo generado correctamente en: data/mapa_calor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d435847e-58df-4ff0-abf1-83a7fa0b2bb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mst\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "st.set_page_config(page_title=\"Dashboard Agente de Seguros\", layout=\"wide\")\n",
    "\n",
    "st.title(\"📊 Inteligencia Comercial del Agente de Seguros\")\n",
    "\n",
    "# Cargar datos\n",
    "polizas = pd.read_pickle('data/polizas.pkl')\n",
    "reclamos = pd.read_pickle('data/reclamaciones.pkl')\n",
    "clientes = pd.read_pickle('data/clientes.pkl')\n",
    "\n",
    "st.header(\"🔎 KPIs Principales\")\n",
    "st.metric(\"Clientes únicos\", polizas['ID_Cliente'].nunique())\n",
    "st.metric(\"Total de Pólizas\", len(polizas))\n",
    "st.metric(\"Prima Total\", f\"${polizas['Prima_Anual'].sum():,.2f}\")\n",
    "\n",
    "# Visualización por tipo de seguro\n",
    "st.subheader(\"📘 Distribución por tipo de seguro\")\n",
    "tipo_data = polizas.groupby('Tipo_Seguro')['Prima_Anual'].sum().sort_values()\n",
    "st.bar_chart(tipo_data)\n",
    "\n",
    "# Cargar modelo de churn\n",
    "model = joblib.load('modelos/churn.pkl')\n",
    "st.subheader(\"⚠️ Clientes con riesgo de fuga (Churn)\")\n",
    "st.write(\"Aquí podrías mostrar tabla de clientes ordenada por probabilidad de fuga.\")\n",
    "\n",
    "# Zonas críticas\n",
    "st.subheader(\"🌍 Mapa de zonas con alta siniestralidad vs baja cobertura\")\n",
    "zonas = pd.read_csv('data/mapa_calor.csv')\n",
    "st.dataframe(zonas.sort_values('Tasa_Siniestro', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c03e5178-0923-4b16-a027-4f03a9a41592",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mst\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Streamlit instalado correctamente.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "print(\"✅ Streamlit instalado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f047936-17e9-4813-a467-a5becbcfc8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 23:19:50.673 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:50.675 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.615 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\ibarr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-07-08 23:19:51.616 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.617 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.627 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.628 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.629 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.645 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.650 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.653 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.654 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.657 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.658 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:51.658 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:52.763 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:52.764 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:52.766 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.694 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.696 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.697 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.698 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.698 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.699 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.700 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.700 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.701 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.720 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:19:56.723 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "st.set_page_config(page_title=\"Dashboard Agente de Seguros\", layout=\"wide\")\n",
    "\n",
    "st.title(\"📊 Inteligencia Comercial del Agente de Seguros\")\n",
    "\n",
    "# Cargar datos\n",
    "polizas = pd.read_pickle('data/polizas.pkl')\n",
    "reclamos = pd.read_pickle('data/reclamaciones.pkl')\n",
    "clientes = pd.read_pickle('data/clientes.pkl')\n",
    "\n",
    "st.header(\"🔎 KPIs Principales\")\n",
    "st.metric(\"Clientes únicos\", polizas['ID_Cliente'].nunique())\n",
    "st.metric(\"Total de Pólizas\", len(polizas))\n",
    "st.metric(\"Prima Total\", f\"${polizas['Prima_Anual'].sum():,.2f}\")\n",
    "\n",
    "# Visualización por tipo de seguro\n",
    "st.subheader(\"📘 Distribución por tipo de seguro\")\n",
    "tipo_data = polizas.groupby('Tipo_Seguro')['Prima_Anual'].sum().sort_values()\n",
    "st.bar_chart(tipo_data)\n",
    "\n",
    "# Cargar modelo de churn\n",
    "model = joblib.load('modelos/churn.pkl')\n",
    "st.subheader(\"⚠️ Clientes con riesgo de fuga (Churn)\")\n",
    "st.write(\"Aquí podrías mostrar tabla de clientes ordenada por probabilidad de fuga.\")\n",
    "\n",
    "# Zonas críticas\n",
    "st.subheader(\"🌍 Mapa de zonas con alta siniestralidad vs baja cobertura\")\n",
    "zonas = pd.read_csv('data/mapa_calor.csv')\n",
    "st.dataframe(zonas.sort_values('Tasa_Siniestro', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa73cbd5-7e7b-4bfc-99cf-62efd25613d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 23:25:13.478 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.512 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.516 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.523 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.699 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.762 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.766 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.769 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.772 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.776 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.777 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.802 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:13.809 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.387 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.390 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.557 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.561 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.563 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.567 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.571 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.574 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.576 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.576 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.578 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.640 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.642 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:25:14.645 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "st.set_page_config(page_title=\"Dashboard Agente de Seguros\", layout=\"wide\")\n",
    "\n",
    "st.title(\"📊 Inteligencia Comercial del Agente de Seguros\")\n",
    "\n",
    "# Cargar datos\n",
    "polizas = pd.read_pickle('data/polizas.pkl')\n",
    "reclamos = pd.read_pickle('data/reclamaciones.pkl')\n",
    "clientes = pd.read_pickle('data/clientes.pkl')\n",
    "\n",
    "st.header(\"🔎 KPIs Principales\")\n",
    "st.metric(\"Clientes únicos\", polizas['ID_Cliente'].nunique())\n",
    "st.metric(\"Total de Pólizas\", len(polizas))\n",
    "st.metric(\"Prima Total\", f\"${polizas['Prima_Anual'].sum():,.2f}\")\n",
    "\n",
    "# Visualización por tipo de seguro\n",
    "st.subheader(\"📘 Distribución por tipo de seguro\")\n",
    "tipo_data = polizas.groupby('Tipo_Seguro')['Prima_Anual'].sum().sort_values()\n",
    "st.bar_chart(tipo_data)\n",
    "\n",
    "# Cargar modelo de churn\n",
    "model = joblib.load('modelos/churn.pkl')\n",
    "st.subheader(\"⚠️ Clientes con riesgo de fuga (Churn)\")\n",
    "st.write(\"Aquí podrías mostrar tabla de clientes ordenada por probabilidad de fuga.\")\n",
    "\n",
    "# Zonas críticas\n",
    "st.subheader(\"🌍 Mapa de zonas con alta siniestralidad vs baja cobertura\")\n",
    "zonas = pd.read_csv('data/mapa_calor.csv')\n",
    "st.dataframe(zonas.sort_values('Tasa_Siniestro', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e959232-c2d7-4998-b30b-ad5fa68501de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 23:26:43.332 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:26:43.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-08 23:26:43.335 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Agrupar datos\n",
    "tipo_data = polizas['Tipo_Seguro'].value_counts()\n",
    "\n",
    "# Mostrar como gráfico de barras\n",
    "st.bar_chart(tipo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea15a0-d5e4-49ec-9518-05ef8dd13ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
